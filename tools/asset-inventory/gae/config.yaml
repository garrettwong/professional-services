# Copyright 2019 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#            http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Where the export is written to. A GCS path like gs://my-bucket-name/asset_export
gcs_destination: <ENTER-BUCKET-URL>

# Organization number (organizations/123) or project id (projects/id) or number (projects/123)
export_parent: <ENTER-PARENT>

# BigQuery dataset to load to.
import_dataset: <ENTER-DATASET>

# Location to write intermediary data to load from from. A GCS path like gs://my-bucket-name/asset_export/stage
import_stage: <ENTER-STAGE>

# Project id to run the dataflow job in when using the template or dataflow runner.
import_dataflow_project: <ENTER-PROJECT>

# Export resources, iam_policy or both.
export_content_types:
- RESOURCE
- IAM_POLICY

# List of asset types to export such as google.compute.Firewall and google.compute.HealthCheck default is '*' for everything.
export_asset_types:
- '*'

# Can be direct, dataflow or template (however App Engine Standard doesn't support the direct or dataflow runner).
import_pipeline_runner: template

# Region to run the dataflow pipeline in.
import_template_region: us-central1

# If running a template. This is the template location.
import_template_location: gs://professional-services-tools-asset-inventory/latest/import_pipeline

# How to group exported resources into Bigquery tables. Either ASSET_TYPE_VERSION or ASSET_TYPE
import_group_by: ASSET_TYPE

# If tables should be ovewritten (WRITE_EMPTY) or appended (WRITE_APPEND) to.
import_write_disposition: WRITE_APPEND

# If we are running on App Engine and only want to be invoked by cron tasks for security reasons.
restrict_to_cron_tasks: True

# When using direct or dataflow running these are the  other arguments to supply to import pipeline.
import_pipeline_arguments: --max_num_workers=10

# When using the template runner. Supply values as a json representation of RuntimeEnvironment:  https://cloud.google.com/dataflow/docs/reference/rest/v1b3/RuntimeEnvironment
import_pipeline_runtime_environment: >
  {
    "maxWorkers": 10
  }

# For the brave why wish to try running the python direct runner on python3.
# beam_experimental_py3: True
